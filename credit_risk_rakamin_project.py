# -*- coding: utf-8 -*-
"""Credit Risk_Rakamin_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GIAQICtmlriTOQ2iVEflEP6IflTP1lpP

# Credit Risk Classification

## Latar Belakang

Dalam industri multifinance, penilaian dan pengelolaan risiko kredit merupakan aspek penting untuk menjaga keberlanjutan bisnis. Risiko kredit yang muncul akibat kegagalan peminjam dalam memenuhi kewajibannya dapat berdampak besar pada profitabilitas dan stabilitas perusahaan. Oleh karena itu, kemampuan untuk menilai risiko kredit secara akurat sangat diperlukan untuk mengoptimalkan keputusan bisnis dan meminimalkan potensi kerugian.

Sebagai Data Scientist di ID/X Partners, saya diberi kesempatan untuk berkontribusi dalam sebuah proyek strategis bersama salah satu perusahaan pemberi pinjaman (multifinance). Perusahaan ini memiliki tujuan utama untuk meningkatkan keakuratan sistem penilaian risiko kredit mereka. Dengan memanfaatkan data historis pinjaman yang telah disediakan, pada proyek ini saya bertujuan untuk mengembangkan model machine learning yang mampu memprediksi risiko kredit secara efektif.

Melalui pendekatan berbasis data ini, saya berharapkan perusahaan dapat membuat keputusan yang lebih informasional dalam menyetujui atau menolak aplikasi pinjaman, mengurangi risiko gagal bayar, dan pada akhirnya meningkatkan efisiensi bisnis mereka.

## Informasi Dataset

Diketahui bahwa dataset yang diberikan adalah sebagai berikut :
- member_id: ID unik LC untuk setiap anggota peminjam.
- loan_amnt: Pembayaran yang diterima bulan lalu.
- funded_amnt: Komitmen total pembayaran setiap bulannya.
- funded_amnt_inv: Komitmen total yang didanai oleh investor(bank) individu untuk pinjaman.
- term: Berapa bulan ia meminjam.
- int_rate: Tingkat rating yang diverifikasi LC.
- installment: Jumlah pembayaran bulanan jika pinjaman diajukan.
- grade: Peringkat risiko yang diberikan kepada peminjam berdasarkan analisis skor kredit dan faktor keuangan lainnya.
- sub_grade: Subdivisi lebih detail dari grade, memberikan gambaran lebih rinci mengenai profil risiko peminjam.
- emp_title: Pekerjaan anggota peminjam.
- emp_length: Lama bekerja.
- home_ownership: Status tempat tinggal.
- annual_inc: Penghasilan tahunan.
- verification_status: Status apakah sudah diverifikasi.
- issue_d: Bulan pinjaman didanai.
- loan_status: Status pinjaman saat ini.
- pymnt_plan: Perencanaan pembayaran.
- url: Web ID.
- desc: Deskripsi alasan peminjaman.
- purpose: Kategori peminjaman.
- title: Judul pinjaman.
- zip_code: Kode pos.
- addr_state: Alamat negara.
- dti: Rasio total utang terhadap pendapatan bulanan peminjam.
- delinq_2yrs: Jumlah pelanggaran 30+ hari dalam 2 tahun terakhir.
- earliest_cr_line: Bulan pertama pembukaan jalur kredit.
- inq_last_6mths: Jumlah pemeriksaan kredit dalam 6 bulan terakhir.
- mths_since_last_delinq: Jumlah bulan sejak pelanggaran terakhir.
- mths_since_last_record: Jumlah bulan sejak catatan publik terakhir.
- open_acc: Jumlah jalur kredit aktif.
- pub_rec: Catatan publik derogatori.
- revol_bal: Total saldo kredit bergulir.
- revol_util: Rasio penggunaan saldo kredit bergulir.
- total_acc: Jumlah kredit dalam file kredit.
- initial_list_status: Status awal peminjam.
- out_prncp: Sisa pokok pinjaman untuk total pendanaan.
- out_prncp_inv: Sisa pokok pinjaman untuk investor.
- total_pymnt: Total pembayaran hingga saat ini.
- total_pymnt_inv: Pembayaran diterima oleh investor hingga saat ini.
- total_rec_prncp: Modal yang diterima hingga saat ini.
- total_rec_int: Bunga yang diterima hingga saat ini.
- total_rec_late_fee: Biaya keterlambatan yang diterima hingga saat ini.
- recoveries: Rencana pembayaran dibuat untuk pinjaman.
- collection_recovery_fee: Biaya pengumpulan dari biaya penagihan.
- last_pymnt_d: Tanggal pembayaran terakhir.
- last_pymnt_amnt: Jumlah pembayaran terakhir.
- next_pymnt_d: Tanggal pembayaran berikutnya.
- last_credit_pull_d: Kapan terakhir kali kredit diperiksa.
- collections_12_mths_ex_med: Jumlah koleksi kredit 12 bulan terakhir (tidak termasuk medis).
- mths_since_last_major_derog: Bulan sejak peringkat buruk terakhir (90+ hari).
- policy_code: Kode kebijakan.
- application_type: Jenis aplikasi (individu atau bersama).
- annual_inc_joint: Gabungan pendapatan tahunan peminjam bersama.
- dti_joint: Rasio DTI gabungan untuk peminjam bersama.
- verification_status_joint: Status verifikasi pendapatan gabungan.
- acc_now_delinq: Jumlah akun yang menunggak.
- tot_coll_amt: Total jumlah koleksi yang pernah ada.
- tot_cur_bal: Total saldo saat ini dari semua akun.
- open_acc_6m: Jumlah perdagangan yang dibuka dalam 6 bulan terakhir.
- open_il_6m: Jumlah akun cicilan yang dibuka dalam 6 bulan terakhir.
- open_il_12m: Jumlah akun cicilan yang dibuka dalam 12 bulan terakhir.
- open_il_24m: Jumlah akun cicilan yang dibuka dalam 24 bulan terakhir.
- mths_since_rcnt_il: Bulan sejak akun cicilan terakhir dibuka.
- total_bal_il: Total saldo saat ini untuk semua akun cicilan.
- il_util: Rasio saldo terhadap batas kredit pada akun cicilan.
- open_rv_12m: Jumlah akun revolving dibuka dalam 12 bulan terakhir.
- open_rv_24m: Jumlah akun revolving dibuka dalam 24 bulan terakhir.
- max_bal_bc: Saldo maksimum akun revolving.
- all_util: Saldo terhadap batas kredit pada semua perdagangan.
- total_rev_hi_lim: Total kredit bergulir maksimum.
- inq_fi: Jumlah pertanyaan keuangan pribadi.
- total_cu_tl: Jumlah perdagangan keuangan.
- inq_last_12m: Jumlah pemeriksaan kredit dalam 12 bulan terakhir.

## Analisis Data

Terdapat beberapa tahapan dalam menganalisis data kredit pada project ini, diantaranya:
1. Data Understanding: Analisis awal data untuk memahami struktur, distribusi, dan karakteristik utama.
2. Exploratory Data Analysis (EDA): Visualisasi data, analisis korelasi, univariat, dan bivariat.
3. Data Preparation: Menangani nilai hilang, outlier, encoding data kategorikal, scaling, dan membagi dataset.
4. Modeling: Pilih minimal dua algoritma (termasuk Logistic Regression sebagai wajib) untuk membangun model.
5. Evaluation: Evaluasi performa model menggunakan metrik seperti akurasi, presisi, recall, dan ROC-AUC.

Berikut adalah Analisis Data

### 1. Data Understanding

Proses identifikasi awal dari dataset yang diberikan untuk mengenali karakteristik dataset.

#### a. Importing Libraries & Dataset
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv("loan_data_2007_2014 (1).csv")
profile_data = pd.read_excel("LCDataDictionary (1).xlsx")

data.head()

data.shape

"""Diketahui : terdapat 75 kolom (fitur) dan 466285 baris (observasi)

#### b. Identifikasi Struktur Dataset
"""

data.info()

# Fungsi 'unique()' pada suatu series dalam library pandas
# digunakan untuk mengidentifikasi nilai-nilai unik yang terdapat dalam series tersebut.
# Dalam kasus ini dapat digunakan untuk memeriksa DataFrame yang berisi data string.

data.loan_status.unique()

# Syntax tersebut digunakan untuk membuat kolom baru bernama 'good_bad' dalam DataFrame loan_data.
# Kolom ini bertujuan untuk melihat kolom 'good_bad' yang berisikan data numerik '0' = pembayaran tepat waktu
# dan '1' = pembayaran tidak tepat waktu.

data['good_bad'] = np.where(data.loc[:, 'loan_status'].isin(['Charged Off',
                                                                   'Default', 'Late (31-120 days)', 'Late (16-30 days)'])
                                 , 1, 0)

# Berikut adalah banyaknya nilai data yang 'pembayaran tepat waktu' dan 'pembayaran tidak tepat waktu'

data.good_bad.value_counts()

"""Karena data target memiliki label, maka tujuan dari prediksi ini adalah memprediksi data target, yaitu `loan_status` yang ditransformasi menjadi berbentuk biner berlabel `good_bad` dengan data **0** untuk pembayaran tepat waktu dan  data **1** untuk pembayaran tidak tepat waktu.

Aturan dari pembentukkan fitur good_bad adalah memasukkan seluruh fitur **'Charged Off', 'Default', 'Late (31-120 days)', 'Late (16-30 days)'** ke 1 dan selain itu ke 0.

#### c. Handling Missing Values
"""

# - data.isnull().sum(): Menghitung jumlah nilai-nilai yang hilang (NaN) dalam setiap kolom DataFrame loan_data.
# - / data.shape[0]: Membagi setiap jumlah nilai-nilai yang hilang dengan jumlah baris total dalam DataFrame
# (data.shape[0]) untuk mendapatkan proporsi nilai-nilai yang hilang relatif terhadap jumlah baris.
# - pd.DataFrame(...): Membuat DataFrame baru dari hasil perhitungan di atas.

missing_values = pd.DataFrame(data.isnull().sum()/data.shape[0])
missing_values

# - missing_values.iloc[:, 0]: Memilih nilai-nilai dalam kolom pertama dari DataFrame
# missing_values (yaitu, kolom yang berisi proporsi nilai-nilai yang hilang).
# - [...] > 0.50: Membuat kondisi yang memilih hanya baris-baris di mana proporsi nilai-nilai yang hilang lebih dari 50%.
# - missing_values[...]: Memilih hanya baris-baris yang memenuhi kondisi tersebut, sehingga menghasilkan DataFrame baru yang
# hanya berisi kolom-kolom yang memiliki lebih dari 50% nilai-nilai yang hilang.

missing_values = missing_values[missing_values.iloc[:,0] > 0.50]

# - missing_values.sort_values([0], ascending=False): Mengurutkan DataFrame missing_values berdasarkan nilai
# kolom pertama (0), yang merupakan proporsi nilai-nilai yang hilang.
# - ascending=False: Mengurutkan secara menurun, sehingga kolom dengan proporsi nilai-nilai
# yang hilang terbesar akan berada di bagian atas DataFrame.

missing_values.sort_values([0], ascending=False)

"""Fitur (kolom) dengan data kosong lebih dari **50%** akan dihapus karena jika diimputasi akan menyebabkan **‘bias’**."""

# - data.shape[0]: Mengambil jumlah baris dalam DataFrame loan_data.
# - how='all': Ini berarti kolom akan dihapus hanya jika seluruh baris dalam kolom tersebut memiliki nilai yang hilang.
# - axis=1: Menunjukkan bahwa kita ingin melakukan operasi pada kolom (bukan baris).
# - inplace=True: Menunjukkan bahwa perubahan akan dilakukan langsung pada DataFrame
# data tanpa perlu menyimpan hasilnya ke variabel baru.

data.dropna(thresh = data.shape[0]*0.5 , axis=1 , inplace=True)

missing_values = pd.DataFrame(data.isnull().sum()/data.shape[0])
missing_values = missing_values[missing_values.iloc[:,0] > 0.50]
missing_values.sort_values([0], ascending=False)

"""sekarang data dengan missing values diatas 50% sudah tidak ada."""

data = data.drop(columns=['url'])

"""#

### 2. Exploratory Data Analysis (EDA)

Eksplorasi dataset dengan menggunakan analisis lanjutan serta pemahaman statistik.

#### a. Descriptive Statistics
"""

data.info()

data.describe()

"""Karena data terlalu banyak, data akan dibersihan untuk mendapatkan fitur yang paling penting."""

# List nama kolom yang ingin dihapus
columns_to_drop = ["grade",
                "sub_grade",
                  "home_ownership",
                  "verification_status",
                  "purpose",]

# Hapus kolom dari dataset
data = data.drop(columns=columns_to_drop)

# Cek hasil
data.info()

# Cek apakah masih ada pinjaman yang memiliki pembayaran berikutnya
print(data['next_pymnt_d'].notnull().sum())

# Statistik berdasarkan good_bad
print(data.groupby('good_bad')['next_pymnt_d'].describe())

"""Berdasarkan analisis, diketahui bahwa terdapat **239.071** kreditur yang akan membayar pada pembayaran berikutnya `(next_pymnt_d)`."""

# Statistik dasar terhadap pembayaran terakhir
print(data['last_pymnt_d'].describe())

# Distribusi berdasarkan target
print(data.groupby('good_bad')['last_pymnt_d'].describe())

"""Mayoritas kreditur terakhir membayar pada **Januari 2016**. Hal ini berarti sebagian besar pinjaman memiliki aktivitas pembayaran terakhir di bulan yang sama."""

# Statistik dasar
print(data['last_credit_pull_d'].describe())

# Distribusi berdasarkan good_bad
print(data.groupby('good_bad')['last_credit_pull_d'].describe())

"""- Peminjam yang gagal bayar (good_bad = 1) hanya memiliki **51.419** entri (sekitar 11%), yang menunjukkan bahwa mungkin tidak lagi diawasi secara aktif setelah melewati titik tertentu.
- Peminjam dengan status good_bad = 0 (tidak gagal bayar) memiliki **103** nilai unik dalam last_credit_pull_d, menunjukkan bahwa pemeriksaan kredit dilakukan secara lebih bervariasi sepanjang waktu.

#### b. Data Cleansing
"""

data.shape

# Pada fungsi perulangan 'for' berikut, akan dimunculkan kolom-kolom yang bertipe 'object' dan 'bool'

for col in data.select_dtypes(include = ['object','bool']).columns :
    print(col)
    print(data[col].unique())
    print()

# Dibentuk variabel 'col_need_to_clean' untuk membentuk
# data-data yang dapat diukur dengan mengubahnya menjadi data numerik
# namun, data-data yang diubah adalah ordinal karena memiliki urutan/hierarki

col_need_to_clean = ['term', 'emp_length', 'issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d'
                    , 'last_credit_pull_d']

# Pembersihan pertama, dimulai dengan kolom 'term'

data['term'].unique()

# Pembersihan pertama, dimulai dengan kolom 'term'
# Kolom 'term' akan dirubah kedalam bentuk numerik dan akan dihilangkan stringnya

data['term'].unique()
data['term'] = pd.to_numeric(data['term'].str.replace(' months', ''))

# Dapat dilihat bahwa kolom 'term' sudah dalam bentuk numerik
data['term']

data['term'].unique()

# Pembersihan kedua, dilanjutkan dengan kolom 'emp_length'

print(data['emp_length'].unique())

# Kolom 'emp_length' akan dirubah kedalam bentuk numerik dan akan dihilangkan stringnya

data['emp_length'] = data['emp_length'].str.replace('\+ years', '')
data['emp_length'] = data['emp_length'].str.replace(' years', '')
data['emp_length'] = data['emp_length'].str.replace('< 1 year', '0')
data['emp_length'] = data['emp_length'].str.replace(' year', '')


data['emp_length'].fillna(value = 0, inplace=True)

data['emp_length'].unique()

# Terlihat bahwa masih ada data non numerik '10+'

# Berikut akan dihilangkan tanda '+' dan merubah jadi numerik

data['emp_length'] = data['emp_length'].str.replace('+', '')
data['emp_length'] = pd.to_numeric(data['emp_length'])
data['emp_length']

data['emp_length'] = data['emp_length'].fillna(0.0)

# Berikut menghilangkan data 'NaN'dan merubah keseluruhan data menjadi integer
data['emp_length'] = data['emp_length'].astype(int)
data['emp_length'].unique()

# Berikutnya, akan dibentuk variabel dengan time data timedate

col_date = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d',
            'last_credit_pull_d']

data[col_date]

# Melihat apakah ada baris yang kosong
for n in col_date:
    print(n , data[n].isnull().sum())

# Menghapus baris yang memiliki NaN
for m in col_date:
    data = data.dropna(subset=[m])
    print(m , data[m].isnull().sum())

# Menambahkan tahun '20' di depan angka tahun untuk mengubahnya menjadi 4 digit
for col in col_date:
    data[col] = data[col].apply(lambda x: x.replace(f"-{x[-2:]}", f"-20{x[-2:]}"))

# Merubah col_date menjadi tipe 'datetime'
for col in col_date :
    data[col] = pd.to_datetime(data[col])

# Dapat dilihat bahwa kolom 'issue_d', 'last_pymnt_d', 'next_pymnt_d', dan 'last_credit_pull_d' sudah bertipe data datetime

data[col_date]

"""#### c. Inference Statistics

> ### Univariate Analysis

Secara umum, univariate merupakan analisis data yang hanya melibatkan satu variabel pada satu waktu.
"""

# Buat subplots
num_cols = len(col_need_to_clean)
fig, axes = plt.subplots(nrows=1, ncols=num_cols, figsize=(num_cols * 4, 4))  # Grid horizontal

# Loop untuk setiap kolom
for i, col in enumerate(col_need_to_clean):
    axes[i].hist(data[col].dropna(), bins=30, color='royalblue', alpha=0.7)
    axes[i].set_title(f"Distribusi {col}", fontsize=10)
    axes[i].tick_params(axis='x', rotation=45, labelsize=8)
    axes[i].set_ylabel("Jumlah Observasi", fontsize=8)

plt.tight_layout()
plt.show()

"""#### Insight :
1. Distribusi `term` : Mayoritas pinjaman memiliki jangka waktu 36 bulan, tetapi cukup banyak juga yang memiliki tenor 60 bulan.
2. Distribusi `emp_length` (Lama Bekerja) : Banyak peminjam memiliki pengalaman kerja lebih dari 10 tahun, yang bisa mengindikasikan kestabilan finansial lebih tinggi.
3. Distribusi `issue_d` (Tanggal Penerbitan Pinjaman) : Mayoritas pinjaman diterbitkan dalam rentang waktu 2012-2015, dengan peningkatan jumlah penerbitan seiring waktu. Bisa dilakukan **analisis tren**: apakah terjadi peningkatan atau penurunan dalam jumlah pinjaman yang diberikan setiap tahun.
4. Distribusi `earliest_cr_line` (Tahun Pembukaan Kredit Pertama) : Banyak peminjam memiliki riwayat kredit sejak 2000-an, dengan jumlah peminjam menurun untuk tahun-tahun lebih lama. Bisa dibandingkan dengan tingkat gagal bayar untuk melihat apakah peminjam dengan riwayat kredit lebih lama memiliki **risiko lebih rendah**.
5. Distribusi `last_pymnt_d` (Tanggal Pembayaran Terakhir) : Mayoritas pembayaran terakhir terjadi sekitar tahun 2015, menunjukkan bahwa dataset berisi banyak pinjaman yang masih aktif atau baru saja dilunasi. Bisa dilakukan analisis keterlambatan pembayaran dengan membandingkan `last_pymnt_d` dengan `issue_d`.
6. Distribusi `next_pymnt_d` (Tanggal Pembayaran Berikutnya) : Hampir seluruh data menunjukkan nilai pada tahun 2015. Bisa dicek apakah ada peminjam yang seharusnya memiliki jadwal pembayaran tetapi tidak ada data `next_pymnt_d`, yang bisa menjadi indikasi kredit macet.
7. Distribusi `last_credit_pull_d` (Tanggal Penarikan Kredit Terakhir) : Hampir semua data menunjukkan nilai pada tahun 2015, yang berarti laporan kredit terbaru dari peminjam sebagian besar diambil pada tahun tersebut.

#### Analisa Lanjutan

> 1. Melihat tren penerbitan pinjaman berdasarkan tahun.
"""

# Plot distribusi jumlah pinjaman per tahun
plt.figure(figsize=(10, 5))
data['issue_d'].dt.year.value_counts().sort_index().plot(kind='bar', color='royalblue')
plt.xlabel("Tahun Penerbitan Pinjaman")
plt.ylabel("Jumlah Pinjaman")
plt.title("Distribusi Penerbitan Pinjaman per Tahun")
plt.show()

"""> 2. Pembandingan tingkat gagal bayar peminjam dengan riwayat kredit"""

# Hitung usia kredit dalam tahun
data['credit_age'] = (data['issue_d'] - data['earliest_cr_line']).dt.days / 365

# Visualisasi: Boxplot usia kredit berdasarkan good_bad
plt.figure(figsize=(12, 6))
sns.boxplot(x='good_bad', y='credit_age', data=data, showfliers=False, palette="coolwarm")
plt.xlabel("Status Pinjaman")
plt.ylabel("Usia Kredit (Tahun)")
plt.title("Usia Kredit vs Status Pinjaman")
plt.xticks(rotation=45)
plt.show()

"""Karena tidak ada perbedaan signifikan, maka usia kredit mungkin bukan faktor utama dalam menentukan risiko gagal bayar.
Akan lebih lanjut dengan statistical test (ANOVA) untuk melihat apakah perbedaannya signifikan.

#### Hipotesis

- H₀ (Hipotesis Nol): Tidak ada perbedaan signifikan dalam usia kredit antara kelompok loan_status.
- H₁ (Hipotesis Alternatif): Setidaknya ada satu kelompok `good_bad` yang memiliki rata-rata usia kredit yang berbeda secara signifikan.
"""

import scipy.stats as stats

# Hapus data NaN untuk analisis yang valid
df = data.dropna(subset=['credit_age', 'good_bad'])

# Buat kelompok usia kredit berdasarkan loan_status
groups = [group['credit_age'].values for name, group in df.groupby('good_bad')]

# Uji ANOVA
f_stat, p_value = stats.f_oneway(*groups)

# Tampilkan hasil
print(f"F-statistic: {f_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Interpretasi hasil
alpha = 0.05
if p_value < alpha:
    print("Hasil: Tolak H₀. Ada perbedaan signifikan dalam usia kredit berdasarkan status pinjaman.")
else:
    print("Hasil: Gagal menolak H₀. Tidak ada perbedaan signifikan dalam usia kredit berdasarkan status pinjaman.")

# Uji ANOVA
f_stat, p_value = stats.f_oneway(*groups)

# Tampilkan hasil
print(f"F-statistic: {f_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Interpretasi hasil
alpha = 0.05
if p_value < alpha:
    print("Hasil: Tolak H₀. Ada perbedaan signifikan dalam usia kredit berdasarkan status pinjaman.")
else:
    print("Hasil: Gagal menolak H₀. Tidak ada perbedaan signifikan dalam usia kredit berdasarkan status pinjaman.")

# Visualisasi dengan boxplot
plt.figure(figsize=(8, 6))
sns.boxplot(x='good_bad', y='credit_age', data=df)
plt.xlabel("Status Pinjaman")
plt.ylabel("Usia Kredit")
plt.title("Boxplot Usia Kredit berdasarkan Status Pinjaman")
plt.show()

# Visualisasi distribusi data
plt.figure(figsize=(8, 6))
sns.histplot(df, x='credit_age', hue='good_bad', element='step', stat='density', common_norm=False, kde=True)
plt.xlabel("Usia Kredit")
plt.ylabel("Kepadatan")
plt.title("Distribusi Usia Kredit berdasarkan Status Pinjaman")
plt.show()

"""> 3.  Analisis keterlambatan pembayaran dengan membandingkan `last_pymnt_d` dengan `issue_d`"""

# Hitung selisih bulan antara last_pymnt_d dan issue_d
data['payment_delay_months'] = (data['last_pymnt_d'].dt.year - data['issue_d'].dt.year) * 12 + \
                               (data['last_pymnt_d'].dt.month - data['issue_d'].dt.month)

# Plot distribusi keterlambatan pembayaran
plt.figure(figsize=(10, 5))
data['payment_delay_months'].hist(bins=30, color='darkorange', edgecolor='black')
plt.xlabel("Lama Pembayaran (Bulan)")
plt.ylabel("Jumlah Peminjam")
plt.title("Distribusi Keterlambatan Pembayaran")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""> 4. Pengecekan apakah ada peminjam yang seharusnya memiliki jadwal pembayaran tetapi tidak ada data `next_pymnt_d` sebagai indikasi kredit macet."""

# Plot distribusi next payment date
plt.figure(figsize=(10, 5))
df['next_pymnt_d'].dt.year.value_counts().sort_index().plot(kind='bar', color='purple')
plt.xlabel("Tahun Pembayaran Berikutnya")
plt.ylabel("Jumlah Peminjam")
plt.title("Distribusi Next Payment Date")
plt.show()

# Cek apakah ada peminjam yang seharusnya memiliki jadwal pembayaran tetapi tidak memiliki next_pymnt_d
missing_next_pymnt = df[df['next_pymnt_d'].isna()]
print(f"Jumlah peminjam tanpa jadwal pembayaran berikutnya: {len(missing_next_pymnt)}")

"""> 5. Distribusi data target `good_bad`"""

# Barplot
sns.countplot(x="good_bad", data=data)
plt.title("Distribution of Loan Status")
plt.show()

"""Dapat dilihat bahwa variabel target terjadi permasalahan 'class imbalance'.

Ketidakseimbangan kelas ini seringkali menjadi masalah dalam model pembelajaran mesin (machine learning), karena model mungkin lebih condong untuk memprediksi kelas mayoritas dan mengabaikan kelas minoritas, yang dapat mempengaruhi akurasi dan performa model, terutama dalam klasifikasi.

Untuk mengatasi hal ini akan digunakan metode SMOTE.

> ### Multivariate Analysis

Sedangkan multivariate adalah analisis data yang melibatkan dua atau lebih variabel secara bersamaan untuk mengidentifikasi hubungan atau pola di antara variabel-variabel tersebut.

#### Korelasi
"""

plt.figure(figsize=(10,6))
sns.heatmap(data[col_need_to_clean].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Korelasi Antar Variabel Waktu dan Status Kredit")
plt.show()

"""#### Analisa Lanjutan

> 1. Payment Date
"""

# Plot scatter
plt.figure(figsize=(10, 6))
plt.scatter(data['last_pymnt_d'], data['next_pymnt_d'], alpha=0.5)

# Label dan judul
plt.xlabel('Last Payment Date')
plt.ylabel('Next Payment Date')
plt.title('Scatter Plot of Payment Dates')
plt.xticks(rotation=45)
plt.show()

"""> 2. Issues and Payments Correlation"""

# Pastikan kolom tanggal sudah dalam format datetime
data['issue_d'] = pd.to_datetime(data['issue_d'])
data['last_pymnt_d'] = pd.to_datetime(data['last_pymnt_d'])
data['next_pymnt_d'] = pd.to_datetime(data['next_pymnt_d'])

plt.figure(figsize=(10, 6))

plt.scatter(data['issue_d'], data['last_pymnt_d'], label='Tanggal Pembayaran Terakhir', alpha=0.5)
plt.scatter(data['issue_d'], data['next_pymnt_d'], label='Tanggal Pembayaran Berikutnya', alpha=0.5)

plt.xlabel('Tanggal Penerbitan Pinjaman')
plt.ylabel('Tanggal Pembayaran')
plt.title('Hubungan antara Tanggal Penerbitan dengan Tanggal Pembayaran')
plt.legend()
plt.grid(True)
plt.show()

"""> 3. Distribution of Term"""

plt.figure(figsize=(10, 6))
plt.hist(data['term'], bins=len(data['term'].unique()), alpha=0.7, edgecolor='black')

plt.xlabel('Term (Jangka Waktu Pinjaman)')
plt.ylabel('Frekuensi')
plt.title('Distribusi Term Pinjaman')

plt.show()

"""#

### 3. Data Preparation

Pemrosesan fitur-fitur yang akan digunakan dalam modelling dilakukan agar data yang digunakan dapat  memberikan model yang optimal.

#### a. Feature Engineering
"""

data = data[['term', 'emp_length', 'issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d','good_bad']]
data.head()

# Data inilah yang akan digunakan untuk prediksi karena data lainnya sudah tidak relevan karena penghapusan pada tahap cleansing

from datetime import date

pd.to_datetime(date.today().strftime('%Y-%m-%d')) - pd.to_datetime('2007-06-01')

# Ini merupakan jumlah hari dari awal data dikumpulkan pada 2007 sampai hari ini

# Dibentuk function untuk mengukur banyak bulan setelah peminjam meminjam uang
# Catatan : mengapa jumlah bulannya terlampau banyak dari kolom 'term', itu dikarenakan memakai acuan tanggal hari ini.
# Sehingga, data nya menjadi bias. Seharusnya, menggunakan acuan tanggal saat dimana pinjaman dibuat
# Namun, sebagai contoh tidak apa-apa

def date_columns(df, column):
    # Tanggal hari ini
    today_date = pd.to_datetime('2017-12-01')

    # Ubah kolom menjadi format datetime, tangani kesalahan dengan 'coerce' untuk nilai invalid
    df[column] = pd.to_datetime(df[column], format="%b-%y", errors='coerce')

    # Periksa dan tangani NaT (Not a Time) dengan menggantinya menjadi tanggal default (misalnya today_date)
    df[column].fillna(today_date, inplace=True)

    # Hitung jumlah bulan sejak tanggal tersebut
    df['mths_since_' + column] = (
        (today_date.year - df[column].dt.year) * 12 + (today_date.month - df[column].dt.month)
    )

    # Pastikan tidak ada nilai negatif
    df['mths_since_' + column] = df['mths_since_' + column].clip(lower=0)

    # Hapus kolom asli
    df.drop(columns=[column], inplace=True)



# Aplikasikan pada X_train
date_columns(data, 'earliest_cr_line')
date_columns(data, 'issue_d')
date_columns(data, 'last_pymnt_d')
date_columns(data, 'last_credit_pull_d')

data['mths_since_earliest_cr_line']

# Hitung median dari kolom 'mths_since_earliest_cr_line' (tanpa memperhitungkan nilai 0)
median_value = data.loc[data['mths_since_earliest_cr_line'] > 0, 'mths_since_earliest_cr_line'].median()

# Ganti nilai 0 dengan median
data['mths_since_earliest_cr_line'] = data['mths_since_earliest_cr_line'].replace(0, median_value)
data

"""Pemrosesan ini digunakan untuk menghitung jumlah bulan sejak suatu tanggal tertentu hingga **1 Desember 2017** dalam dataset.

Setelah itu, jumlah bulan dihitung berdasarkan selisih antara tahun dan bulan dari tanggal tersebut dengan tanggal referensi, lalu nilai negatif dikoreksi menjadi nol.

#### b. Scalling Data
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data[['term','emp_length','mths_since_earliest_cr_line', 'mths_since_issue_d', 'mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d']] = scaler.fit_transform(
    data[['term','emp_length','mths_since_earliest_cr_line', 'mths_since_issue_d', 'mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d']]
)

# Normalisasi dilakukan dengna menggunakan Z-Score

data

"""Z-score berguna untuk menstandarisasi data dengan mengubahnya menjadi distribusi dengan **mean = 0** dan **standar deviasi = 1**. Ini penting untuk algoritma yang sensitif terhadap skala fitur, seperti regresi linier, KNN, SVM, dan PCA, karena memastikan bahwa semua fitur memiliki kontribusi yang seimbang dalam model.

#### c. Splitting Data
"""

# Akan diimport train_test_spli dari library sklearn.model_selection untuk membentuk model machine learning

from sklearn.model_selection import train_test_split

# Data akan dibagi menjadi 80% train set dan 20% test set
# X adalah seluruh kolom dari DataFrame kecuali kolom 'good_bad'
# y adalah kolom 'good_bad'
X = data.drop('good_bad', axis=1)
y = data['good_bad']
# Install library jika belum terpasang
!pip install imbalanced-learn

# Import SMOTE
from imblearn.over_sampling import SMOTE
from collections import Counter

# Lihat distribusi sebelum oversampling
print("Distribusi sebelum SMOTE:", Counter(y))

# Terapkan SMOTE untuk oversampling kelas minoritas
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Lihat distribusi setelah oversampling
print("Distribusi setelah SMOTE:", Counter(y_resampled))

# - 'test_size=0.2', Parameter ini menentukan proporsi data yang akan dialokasikan sebagai test set
# - 'stratify=y', Parameter ini digunakan untuk memastikan bahwa distribusi kelas pada train set
# dan test set tetap sama seperti distribusi kelas pada dataset asli
# - 'random_state=42', Parameter ini memberikan nilai seed untuk generator angka acak
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)

# Berikut adalah perbandingan nilai 'good' (0) dan 'bad' (1) yang sudah displitting pada train set
y_train.value_counts(normalize=True)

# Berikut adalah perbandingan nilai 'good' (0) dan 'bad' (1) yang sudah displitting pada test set
y_test.value_counts(normalize=True)

# Barplot
sns.countplot(x=y_test)
plt.title("Distribution of Loan Status")
plt.show()

y_test.shape

y_train.shape

X_test.shape

X_train.shape

"""#

### 5. Data Modelling

#### a. Importing Models
"""

# Diimport model Logistic Regresi untuk melakukan proses machine learning data

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
#Dibentuk variabel untuk model Logistic Regresi, yaitu 'model'

model = LogisticRegression()
model_2 = RandomForestClassifier()
model_3 = XGBClassifier()

# Dibentuk fitting data dari data train 'X_train' dan 'y_train'
# Dibentuk variabel prediksi yang mempengaruhi berdasarkan variabel independent 'X_test'

model.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred2 = model_2.predict(X_test)
y_pred3 = model_3.predict(X_test)

# Dibentuk variabel 'result' yang berisikan hasil dari machine learning dengan menggunakan model 'LogisticRegression'
# Berdasarkan 5 data pertama, dapat dilihat bahwa 4 dari 5 data tersebut benar diprediksi
# *catatan : 'y_pred' adalah variabel yang dilakukan uji coba dan 'y_test' adalah variabel kunci atau hasil sebenarnya

result = pd.DataFrame(list(zip(y_pred, y_test)), columns = ['y_pred', 'y_test'])
result.head()

"""#### b. Models Accuracy"""

# Lalu, akan dicari skor akurasi prediksi dengan variabel-variabel prediksi yang digabung menjadi 'y_pred'
# Diimport 'accuracy_score' dari sklearn.metrics
# Dapat dilihat bahwa skor akurasinya sekitar 90% atau 0,9...

from sklearn.metrics import accuracy_score
print("Akurasi Logistic Regression :", accuracy_score(y_test, y_pred))
print("Akurasi Random Forest :", accuracy_score(y_test, y_pred2))
print("Akurasi XGBoost :", accuracy_score(y_test, y_pred3))

"""#

### 6. Evaluation

#### a. Confusion Matrix
"""

# Karena hasil akurasi ini masih terlalu general, maka akan digunakan property penguji akurasi yang lainnya
# Diimport 'confusion_matrix' dari sklearn.metrics

from sklearn.metrics import confusion_matrix

# Matriks kebingungan digunakan untuk mengevaluasi kinerja model
# klasifikasi dengan membandingkan nilai sebenarnya (y_test) dan nilai prediksi (y_pred).
cm = confusion_matrix(y_test, y_pred)
cm2 = confusion_matrix(y_test, y_pred2)
cm3 = confusion_matrix(y_test, y_pred3)

# fungsi 'heatmap' dari modul seaborn untuk membuat peta panas dari confusion matrix (cm).
# 'annot=True' digunakan untuk menampilkan nilai di dalam sel,
# 'fmt='.0f'' digunakan untuk mengatur format nilai di dalam sel (dalam hal ini, bilangan bulat tanpa desimal),
# 'cmap=plt.cm.Blues' digunakan untuk mengatur skema warna peta panas

# Membuat figure dengan 3 subplot dalam satu baris
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Plot confusion matrix pertama
sns.heatmap(cm, annot=True, fmt='.0f', cmap=plt.cm.Blues, ax=axes[0])
axes[0].set_title("Confusion Matrix - Model 1")
axes[0].set_xlabel("Predicted")
axes[0].set_ylabel("Actual")

# Plot confusion matrix kedua
sns.heatmap(cm2, annot=True, fmt='.0f', cmap=plt.cm.Blues, ax=axes[1])
axes[1].set_title("Confusion Matrix - Model 2")
axes[1].set_xlabel("Predicted")
axes[1].set_ylabel("Actual")

# Plot confusion matrix ketiga
sns.heatmap(cm3, annot=True, fmt='.0f', cmap=plt.cm.Blues, ax=axes[2])
axes[2].set_title("Confusion Matrix - Model 3")
axes[2].set_xlabel("Predicted")
axes[2].set_ylabel("Actual")

plt.tight_layout()
plt.show()

"""- Logistic Regression cenderung memiliki lebih banyak FN, yang berarti model ini kurang baik dalam mendeteksi kasus positif (kemungkinan gagal mendeteksi risiko kredit tinggi).
- Random Forest meningkatkan TP secara signifikan dibanding Logistic Regression, namun masih memiliki sedikit lebih banyak FP dibanding XGBoost.
- XGBoost memberikan performa terbaik secara keseluruhan, dengan FN lebih sedikit dari Logistic Regression dan FP lebih sedikit dari Random Forest.
"""

plt.hist(y_pred)

# Dapat dilihat bahwa model prediksi dari variabel dependent atau y_pred dapat dikatakan hampir seimbang / balance

"""#### b. Metrics Evaluasi"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Menghitung metrik evaluasi
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

accuracy2 = accuracy_score(y_test, y_pred2)
precision2 = precision_score(y_test, y_pred2)
recall2 = recall_score(y_test, y_pred2)
f1_2 = f1_score(y_test, y_pred2)
roc_auc_2 = roc_auc_score(y_test, y_pred2)

accuracy3 = accuracy_score(y_test, y_pred3)
precision3 = precision_score(y_test, y_pred3)
recall3 = recall_score(y_test, y_pred3)
f1_3 = f1_score(y_test, y_pred3)
roc_auc_3 = roc_auc_score(y_test, y_pred3)

# Menampilkan hasil evaluasi
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\n")

print(f"Accuracy: {accuracy2:.4f}")
print(f"Precision: {precision2:.4f}")
print(f"Recall: {recall2:.4f}")
print(f"F1 Score: {f1_2:.4f}")
print(f"ROC AUC Score: {roc_auc_2:.4f}")
print("\n")

print(f"Accuracy: {accuracy3:.4f}")
print(f"Precision: {precision3:.4f}")
print(f"Recall: {recall3:.4f}")
print(f"F1 Score: {f1_3:.4f}")
print(f"ROC AUC Score: {roc_auc_3:.4f}")
print("\n")

# Menampilkan Classification Report
print("\nClassification Report Logistic Regression:\n", classification_report(y_test, y_pred))
print("\nClassification Report Random Forest:\n", classification_report(y_test, y_pred2))
print("\nClassification Report XGBoost:\n", classification_report(y_test, y_pred3))

"""**Precision** :
- XGBoost memiliki precision tertinggi untuk kelas 1 (0.99), sedangkan Random Forest terbaik untuk kelas 0 (0.93).
- Logistic Regression memiliki precision paling rendah untuk kelas 0 (0.79).

**Recall** :
- XGBoost memiliki recall tertinggi untuk kelas 0 (0.99), sementara Random Forest terbaik untuk kelas 1 (0.93).
- Logistic Regression memiliki recall terendah untuk kelas 1 (0.74), yang berarti banyak False Negative.

**F1-Score** :
- Random Forest memiliki F1-score tertinggi untuk kelas 1 (0.95), menunjukkan keseimbangan precision dan recall yang sangat baik.
- XGBoost juga cukup tinggi, tetapi sedikit lebih rendah dari Random Forest pada kelas 1.
- Logistic Regression memiliki F1-score paling rendah, terutama pada kelas 1 (0.83).
"""

from sklearn.metrics import roc_curve, auc

# Menghitung probabilitas prediksi positif (kelas 1)
y_prob = model.predict_proba(X_test)[:, 1]  # Ambil probabilitas kelas 1

# Menghitung nilai False Positive Rate (FPR) dan True Positive Rate (TPR)
fpr, tpr, _ = roc_curve(y_test, y_prob)

# Menghitung nilai AUC (Area Under Curve)
roc_auc = auc(fpr, tpr)

# Membuat plot kurva ROC
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Garis diagonal
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

# Menghitung probabilitas prediksi positif (kelas 1)
y_prob = model_2.predict_proba(X_test)[:, 1]  # Ambil probabilitas kelas 1

# Menghitung nilai False Positive Rate (FPR) dan True Positive Rate (TPR)
fpr, tpr, _ = roc_curve(y_test, y_prob)

# Menghitung nilai AUC (Area Under Curve)
roc_auc = auc(fpr, tpr)

# Membuat plot kurva ROC
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Garis diagonal
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

# Menghitung probabilitas prediksi positif (kelas 1)
y_prob = model_3.predict_proba(X_test)[:, 1]  # Ambil probabilitas kelas 1

# Menghitung nilai False Positive Rate (FPR) dan True Positive Rate (TPR)
fpr, tpr, _ = roc_curve(y_test, y_prob)

# Menghitung nilai AUC (Area Under Curve)
roc_auc = auc(fpr, tpr)

# Membuat plot kurva ROC
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Garis diagonal
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""#

### 7. Conclusion

- #### Data yang Perlu Diperhatikan :
Berdasarkan exploratory data analysis (EDA), diketahui jumlah pinjaman meningkat signifikan pada tahun 2014. Lalu, diperhatikan bahwa banyak pinjaman mengalami keterlambatan dalam jangka waktu 10-30 bulan. Berdasarkan uji ANOVA terhadap Usia Kredit dan Status Pinjaman, diketahui bahwa Usia Kredit cukup berpengaruh terhadap Status Pinjaman yang buruk dan berpotensi macet.





- #### Model Terbaik :
Berdasarkan analisa terhadap data loan_data_2007_2014.csv, diketahui model dengan akurasi serta evaluasi False Positive, False Negatif, True Positif, dan True Negatif terbaik adalah XGBoost. Jika tujuan analisis adalah meminimalkan risiko kesalahan dalam mendeteksi kredit berisiko tinggi (False Negatives yang rendah lebih penting), maka XGBoost atau Random Forest lebih disarankan dibanding Logistic Regression. Jika ingin lebih ketat dalam mengurangi False Positives (menghindari salah menolak kredit yang seharusnya layak), XGBoost menjadi pilihan terbaik.
"""